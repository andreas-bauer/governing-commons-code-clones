---
title: "03 Added and removed lines model"
author: "Anders Sundelin"
date: "2023-03-19"
output: html_document
params: 
    cache: "../.cache"
    output: "../ownership/output"
    reloo: FALSE
    cores: 2
    threads: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(tidyr)
library(dplyr)
library(brms)
library(bayesplot)
library(patchwork)
```

```{r}
set.seed(12345)
source("ingest_data.R")
MODEL_CACHE <- "added-M_added_removed_lines"
```

# Settings
Models are stored in the following directory, which must exist prior to knitting this document:

```{r}
cat(normalizePath(paste(getwd(), dirname(cachefile(MODEL_CACHE)), sep="/"), mustWork = T))
```

The used cache directory can be controlled via the cache parameter to Rmd - it can be useful to experiment with this parameter if you Knit the document manually in RStudio.

# Added-and-removed-lines model

Our next choice of model introduce two numerical predictors---added and removed lines---in addition to the existing categorical team and repository.
The intercept and removed lines predictor is allowed to vary between teams, and between teams in repositories.

```{r}
d <- data |> select(y=INTROD,
                    A=A,
                    C=C,
                    D=D,
                    R=R,
                    team=committerteam,
                    repo=repo)
formula <- bf(y ~ 1 + A + R + (1 + R | team) + (1 + R | team:repo),
              zi ~ 1 + A + R + (1 + R | team) + (1 + R | team:repo))
get_prior(data=d,
          family=zero_inflated_negbinomial,
          formula=formula)
```
```{r}
priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.25), class = b),
            prior(weibull(2, 0.25), class = sd),
            prior(lkj(2), class = cor),
            prior(normal(0, 0.5), class = Intercept, dpar=zi),
            prior(normal(0, 0.25), class = b, dpar=zi),
            prior(weibull(2, 0.25), class = sd, dpar=zi),
            prior(gamma(0.5, 0.1), class = shape))

(v <- validate_prior(prior=priors,
               formula=formula,
               data=d,
               family=zero_inflated_negbinomial)
)
```

Relative to the intercept-only model, we now need additional prior information.
The article talks about scaling and centering the numerical predictors, which allow us to choose a beta for the slope in a unit-agnostic way. We settle for a $\mathcal{N}(0, 0.25)$ distribution.

Likewise, we need a prior for the correlation between slopes and intercepts.
We use the standard recommended, and moderately strong $LKJ(2)$ prior, which is mildly sceptical of extreme (nearing 0 or 1) correlations between slopes and intercepts.

## Prior Predictive Checks
```{r}
M_ppc <- brm(data = d,
      family = zero_inflated_negbinomial,
      formula = formula,
      prior = priors,
      file = cachefile(paste0("ppc-",MODEL_CACHE)),
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      sample_prior = "only",
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)
```


```{r}
m <- M_ppc
```


```{r prior_predict}
yrep <- posterior_predict(m)
```

### Number of zeros

```{r ppc_zeros}
ppc_stat(y = d$y, yrep, stat = function(y) mean(y == 0)) + ggtitle("Prior predicted proportion of zeros")
```

The overall shape of the prior predicted proportion of zeros are similar to the intercept-only model, but this model is slightly "flatter at the top".
The model expects anything from $40%$ to $100%$ zeros in the data, with the most likely proportion between around $75%--85%$, slightly below the observed value of $96%$.

### Max predicted value

The maximum predicted values are very similar to the intercept-only model, still ranging from the low single digits, up to aobut 150,000, or even 200,000.
The higher ends are unrealistic, but also very unlikely in our model.

```{r ppc_max}
(sim_max <- ppc_stat(y = d$y, yrep, stat = "max") + ggtitle("Prior predicted max values")
)
```

Scaling the prior predictions to more reasonable values show that our model is similar to the intercept-only model, still expects somewhat smaller max values than observed, but is also somewhat more likely to expect values ranging up to 1000 (the histogram there is higher than in the corresponding figure for $\mathcal{M}_0$.

```{r}
sim_max + scale_x_continuous(limits = c(0,1000)) + ggtitle("Prior predicted max values up to 1000")
```

### 99th percentile

The 99th percentile is a also a good fit for our model.

```{r ppc_q99}
ppc_stat(y = d$y, yrep, stat = "q99") + ggtitle("Prior predicted Q99 vs. observed value")
```


### 99th vs 95th percentile

Relative to the intercept-only model, the shape is the same, but there are a somewhat wider range of values (up to about 100 for q99, and 30 for q95).

```{r ppc_2d}
(p <- ppc_stat_2d(d$y, yrep, stat = c("q95", "q99")) + theme(legend.position="bottom") + ggtitle("Prior predicted Q95 vs Q99")
)
```

### Standard deviation

Same situation for the standard deviations---slightly wider range of standard deviations, only considering the priors.

```{r ppc_sd}
(p <- ppc_stat(y = d$y, yrep, stat = "sd") + ggtitle("Prior predicted stddev vs. observed value")
)
```

Zooming in to show the distribution in more detail.
Relative to the intercept model, this model has a slightly wider standard deviation range (more histogram values over 10).

```{r}
p + scale_x_continuous(limits=c(0,30))
```

### Group-level predictions

The group-level data is more spread out (Q99 varies more than for the intercept-only model).
But the observed values are all covered by the group predictions.

```{r ppc_grouped_team}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$team) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per team")
```

Similar settings apply for the repo-level variations. 

```{r ppc_grouped_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$repo) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per repository")
```

```{r model_execution}
M_added_removed_lines <- brm(data = d,
      family = zero_inflated_negbinomial,
      file = cachefile(MODEL_CACHE),
      formula = formula,
      prior = priors,
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)
```

```{r loo}
M_added_removed_lines <- add_criterion(M_added_removed_lines, "loo")
```

```{r}
m <- M_added_removed_lines
```

## Model diagnostics

```{r mcmc_trace}
p <- mcmc_trace(m)
pars <- levels(p[["data"]][["parameter"]])
plots <- seq(1, to=length(pars), by=12)
lapply(plots, function(i) {
  start <- i
  end <- start+11
  mcmc_trace(m, pars = na.omit(pars[start:end]))
  })
```

The "caterpillar plots" also show that the chains mixed well.

```{r, rhat_neff}
mcmc_plot(m, type="rhat")
mcmc_plot(m, type="rhat_hist")
mcmc_plot(m, type="neff")
mcmc_plot(m, type="neff_hist")
```

Both MCMC chains, $\hat{R}$ and $N_{eff}$ ratio looks good.

```{r plot_loo}
loo <- loo(m)
loo
plot(loo)
```

There are 9 outlier points that need to be analysed (e.g. with reloo)

Reloo is disabled by default, but could be enabled by setting the RELOO environment variable (which in turn will set the `reloo` parameter of this document.

```{r reloo, eval=params$reloo}
reloofile <- cachefile(paste0("reloo-", MODEL_CACHE, ".rds"))
if (file.exists(reloofile)) {
    (reloo <- readRDS(reloofile))
} else {
    Sys.time()
    (reloo <- reloo(m, loo, chains=CHAINS, cores=CORES) )
    Sys.time()
    saveRDS(reloo, reloofile)
}
```

Note that the reloo results are cached, and included in the docker image by default.
If you want to reproduce the reloo manually, set a separate cache dir (and mount it into the docker image), and enable the RELOO environment variable.
It will take several hours, so it is best done over night.

```{r plot_reloo, eval=params$reloo}
# plotting the recalculated values
plot(reloo)
# which points have higher pareto_k than 0.5?
influencers <- data[loo::pareto_k_ids(reloo),]
influencers |> group_by(repo,committerteam) |> tally()
```

According to the reloo function, all Pareto-k values are below 0.7, so we should be able to trust the model inferences.

## Posterior predictive checks

```{r posterior_predict}
yrep <- posterior_predict(m)
```

### Posterior proportion of zeros

```{r posterior_zeros}
ppc_stat(y = d$y, yrep, stat = function(y) mean(y == 0))
```

The distribution of zeros is marginally higher than the observed value, but it still encompasses the observed proportion.

### Posterior max value

```{r posterior_max}
(sim_max <- ppc_stat(y = d$y, yrep, stat = "max")
)
```

Relative to the intercept-only model, our model now expects a wider range of values, up to a few thousand duplicates.
But the most likely value is still close to the observed max value (150).

```{r}
sim_max + scale_x_continuous(limits = c(0,1000))
```

When scaling to $0--1000$, we see that the most likely prediction of our model is actually somewhat less than 150 (observed max value). But then the predictions drop off towards 750, or even 1000.

### Posterior standard distribution

```{r posterior_sd}
ppc_stat(y = d$y, yrep, stat = "sd")
```

Same principle as for the other metrics---the standard deviation is somewhat wider than for the intercept-only model.

### Posterior Q99

```{r posterior_q99}
ppc_stat(y = d$y, yrep, stat = function(y) quantile(y, 0.99)) + ggtitle("Posterior predicted Q99")
```

### Posterior Q95 vs Q99

```{r posterior_q95_vs_q99}
ppc_stat_2d(d$y, yrep, stat = c("q95", "q99")) + ggtitle("Posterior predicted Q95 vs Q99")
```

The 95th and 99th quantiles are spot on the observed values of 1 and 5, respectively.
Still, small variations are allowed.

### Posterior grouped predictions

```{r posterior_max_by_team}
ppc_stat_grouped(y = d$y, yrep, stat = "max", group = d$team) + ggtitle("Posterior predictive max observation per team")
```

The max value per team varies between teams, but for most teams the observed value fall reasonably well within the predictions.
Red team  is the exception.
The maximum observation is a volatile metric for right-skewed data like this.

```{r posterior_max_per_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "max", group = d$repo) + ggtitle("Posterior predictive max observation per repo")
```

We see that overall, the model predicts more duplicates in the integration test repository (just like the data says).

```{r posterior_q99_by_team}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$team) + ggtitle("Posterior predictive 99% quartile per team")
```

The 99th percentile value predictions seem very well fitted. Predictions surround the observation nicely, and the uncertainty about the low-contributing UI team is also visible.

```{r posterior_q99_by_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$repo) + ggtitle("Posterior predictive 99% quartile per repo")
```

The same thing applies to the 99th percentile per repository.

### Rootogram

In a rootogram, predictions are placed on the x-axis, and the square root of the expected frequency is placed on the y axis.
In the suspended style, the difference is shown as a bar chart protuding from the $x$-axis, for each predicted value.

```{r rootogram}
(rootogram <- pp_check(m, type = "rootogram", style="suspended")
)
```

The full scale rootogram does not say much, we need to scale it to reasonable values.

```{r}
rootogram + scale_x_continuous(limits=c(0,50)) + ggtitle("Suspended rootogram, scaled to 0-50 prediction interval")
```

## Predictions

```{r}
source("predict_utils.R")
```

```{r heatmap}
# the predict functions use all predictors, so we have to supply them, even though this model only considers added and removed lines
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q95(data$ADD), removed=q95(data$DEL), complexity=q95(data$COMPLEX), duplicates = q95(data$DUP), summary=function(x) q99(x)), "Quantile 99%", decimals=0) + ggtitle("Quantile 99% per team and repo")
```

Relative to the intercept-only model, this model show more team-level variations (meaning that the team tendency of introducing duplicates, relative to their added and removed lines, is considered).
For instance, Architects are now low ranked, while Brown and Orange are high ranked of the common teams.
The more sporadic teams, UI, Pink and Unknown are also expected to introduce more duplicates overall.

## Conclusion

Our model converges well, but is limited in its predictive capabilities.
We will use it for comparisons with the full model.
