---
title: "03 Added and removed lines model"
author: "Anders Sundelin"
date: "2023-03-19"
output: html_document
params: 
    cache: "../.cache"
    output: "../ownership/output"
    reloo: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(tidyr)
library(dplyr)
library(brms)
library(bayesplot)
library(patchwork)
```

```{r}
set.seed(12345)
source("ingest_data.R")
MODEL_CACHE <- "added-M_added_removed_lines"
```

# Settings
Models are stored in the following directory, which must exist prior to knitting this document:

```{r}
cat(normalizePath(paste(getwd(), dirname(cachefile(MODEL_CACHE)), sep="/"), mustWork = T))
```

The used cache directory can be controlled via the cache parameter to Rmd - it can be useful to experiment with this parameter if you Knit the document manually in RStudio.

# Added-and-removed-lines model

Our next choice of model introduce two numerical predictors---added and removed lines, in addition to the existing categorical team and repository.
The intercept and removed lines predictor is allowed to vary between teams, and between teams in repositories.

```{r}
d <- data |> select(y=INTROD,
                    A=A,
                    C=C,
                    D=D,
                    R=R,
                    team=committerteam,
                    repo=repo)
formula <- bf(y ~ 1 + A + R + (1 + R | team) + (1 + R | team:repo),
              zi ~ 1 + A + R + (1 + R | team) + (1 + R | team:repo))
get_prior(data=d,
          family=zero_inflated_negbinomial,
          formula=formula)
```
```{r}
priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.25), class = b),
            prior(weibull(2, 0.25), class = sd),
            prior(lkj(2), class = cor),
            prior(normal(0, 0.5), class = Intercept, dpar=zi),
            prior(normal(0, 0.25), class = b, dpar=zi),
            prior(weibull(2, 0.25), class = sd, dpar=zi),
            prior(gamma(0.5, 0.1), class = shape))

(v <- validate_prior(prior=priors,
               formula=formula,
               data=d,
               family=zero_inflated_negbinomial)
)
```


```{r}
M_ppc <- brm(data = d,
      family = zero_inflated_negbinomial,
      formula = formula,
      prior = priors,
      file = cachefile(paste0("ppc-",MODEL_CACHE)),
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      sample_prior = "only",
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)
```


```{r}
m <- M_ppc
```


```{r prior_predict}
yrep <- posterior_predict(m)
```

### Number of zeros

```{r ppc_zeros}
ppc_stat(y = d$y, yrep, stat = function(y) mean(y == 0)) + ggtitle("Prior predicted proportion of zeros")
```

### Max predicted value

While our priors place the maximum likely value towards the lower end of the thousands, they do not rule out even 150--200 thousand duplicates in a single change (which we think is quite unrealistic).
But it is important to allow some very unlikely values in the priors, because parameter space where prior information is zero will never show up in the posterior, even if it is present in the data (following Bayes' theorem).
Thus, even though the priors show some unreasonable values, at least we know that if we ever see such data, our model will allow us to see it.

```{r ppc_max}
(sim_max <- ppc_stat(y = d$y, yrep, stat = "max") + ggtitle("Prior predicted max values")
)
```

Scaling to more reasonable values show that our model actually expects somewhat smaller max values than observed, on average - but we see above that it does encompass also the observed max value, and quite a bit more range.

```{r}
sim_max + scale_x_continuous(limits = c(0,1000)) + ggtitle("Prior predicted max values up to 1000")
```

### 99th percentile

The 99th percentile is a more stable metric than the maximum value for highly right-skewed data like ours.
The priors do a good job of matching the observations.

```{r ppc_q99}
ppc_stat(y = d$y, yrep, stat = "q99") + ggtitle("Prior predicted Q99 vs. observed value")
```


### 99th vs 95th percentile

We can even plot the 95th percentile versus the 99th, just to show how the spread of likely values vary.

```{r ppc_2d}
(p <- ppc_stat_2d(d$y, yrep, stat = c("q95", "q99")) + theme(legend.position="bottom") + ggtitle("Prior predicted Q95 vs Q99")
)
```

### Standard deviation

The standard deviation of the predictions is a bit harder to grasp intuitively.
But our prior information encompass the observed value well.

```{r ppc_sd}
(p <- ppc_stat(y = d$y, yrep, stat = "sd") + ggtitle("Prior predicted stddev vs. observed value")
)
```

Zooming in to show the distribution in more detail.
Relative to the intercept model, this model has a slightly wider standard deviation range.

```{r}
p + scale_x_continuous(limits=c(0,30))
```

### Group-level predictions

We can use groups present in the data to show how predictions stack up to observations made per group.
Here, beside showing the distribution, it is also important to consider the scale on the respective $x$-axis.
Teams with few, and varied, observations (such as UI and Unknown) are more reliant on the priors, and will have a larger range of predictions.

```{r ppc_grouped_team}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$team) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per team")
```

Plotting prior predictions per repo can show how our model compares relative to repo-level metrics.
We see that Integration tests are a bit of an anomaly.
Our model does not incorporate any repo-level group (only the team:repo-level grouping, that is, how each team behaves in a particular repo), so all predictions will follow the same distribution.
Note how all $x$-axes are quite similar in scale (the difference comes from random variations, and differing number of data points arising from each reposiotory)

```{r ppc_grouped_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$repo) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per repository")
```

## Model execution and diagnostics

```{r model_execution}
M_added_removed_lines <- brm(data = d,
      family = zero_inflated_negbinomial,
      file = cachefile(MODEL_CACHE),
      formula = formula,
      prior = priors,
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)

```

```{r loo}
M_added_removed_lines <- add_criterion(M_added_removed_lines, "loo")
```

```{r}
m <- M_added_removed_lines
```


```{r mcmc_trace}
p <- mcmc_trace(m)
pars <- levels(p[["data"]][["parameter"]])
plots <- seq(1, to=length(pars), by=12)
lapply(plots, function(i) {
  start <- i
  end <- start+11
  mcmc_trace(m, pars = na.omit(pars[start:end]))
  })
```

```{r, rhat_neff}
mcmc_plot(m, type="rhat")
mcmc_plot(m, type="rhat_hist")
mcmc_plot(m, type="neff")
mcmc_plot(m, type="neff_hist")
```

```{r plot_loo}
loo <- loo(m)
loo
plot(loo)
```

There are 8 outlier points that need to be analysed (e.g. with reloo)

```{r reloo, eval=params$reloo}
Sys.time()
(reloo <- reloo(m, loo, chains=CHAINS, cores=CORES)
)
Sys.time()
saveRDS(reloo, cachefile(paste0("reloo-", MODEL_CACHE, ".rds")))
```

## Posterior predictive checks

```{r posterior_predict}
yrep <- posterior_predict(m)
```

### Posterior proportion of zeros

```{r posterior_zeros}
ppc_stat(y = d$y, yrep, stat = function(y) mean(y == 0))
```

The distribution of zeros are spot-on.

### Posterior max value

```{r posterior_max}
(sim_max <- ppc_stat(y = d$y, yrep, stat = "max")
)
```

```{r}
sim_max + scale_x_continuous(limits = c(0,1000))
```

### Posterior standard distribution

```{r posterior_sd}
ppc_stat(y = d$y, yrep, stat = "sd")
```

### Posterior Q99

```{r posterior_q99}
ppc_stat(y = d$y, yrep, stat = function(y) quantile(y, 0.99)) + ggtitle("Posterior predicted Q99")
```

### Posterior Q95 vs Q99

```{r posterior_q95_vs_q99}
ppc_stat_2d(d$y, yrep, stat = c("q95", "q99")) + ggtitle("Posterior predicted Q95 vs Q99")
```

### Posterior grouped predictions

```{r posterior_max_by_team}
ppc_stat_grouped(y = d$y, yrep, stat = "max", group = d$team) + ggtitle("Posterior predictive max observation per team")
```

The max value per team varies between teams, but for most teams the observed value fall reasonably well within the predictions.
Red team  is the exception.
Remember, this model does not take any numerical predictor (such as size of the change) into account, so different team behaviour will not be visible.

```{r posterior_max_per_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "max", group = d$repo) + ggtitle("Posterior predictive max observation per repo")
```

Posterior max per repository have some variation, in particular in Saturn and Jupiter.

```{r posterior_q99_by_team}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$team) + ggtitle("Posterior predictive 99% quartile per team")
```

The 99th percentile value predictions seem very well fitted. Predictions surround the observation nicely.

```{r posterior_q99_by_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$repo) + ggtitle("Posterior predictive 99% quartile per repo")
```

Rootogram, full scale

```{r rootogram}
(rootogram <- pp_check(m, type = "rootogram", style="suspended")
)
```

Rootogram, sized according to reasonable (observed) values.

```{r}
rootogram + scale_x_continuous(limits=c(0,50)) + ggtitle("Suspended rootogram, scaled to 0-50 prediction interval")
```


## Posterior predictions

The posterior predictions in general work well. Though the single outlier in Saturn is not picked up (as most other changes there yield very few duplicates). Though our priors does allow the outliers to shine though, they do relegate them as very unlikely (most max predictions are in the 10s or 20s, most).

```{r, eval=FALSE}
source("predict_utils.R")
```

```{r, eval=FALSE}
p <- heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=mean(data$ADD), removed=mean(data$DEL), complexity = mean(data$COMPLEX), duplicates = mean(data$DUP)), "probability of zero introduced duplicates", decimals=2)
p

```

```{r, eval=FALSE}
p <- heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q95(data$ADD), removed=q95(data$DEL)), "proportion zeros", decimals=2)
p

```

```{r, eval=FALSE}
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q95(data$ADD), removed=q95(data$DEL), summary=function(x) q95(x)), "Quantile 95%", decimals=0)
```

```{r, eval=FALSE}
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q95(data$ADD), removed=q95(data$DEL), summary=function(x) q99(x)), "Quantile 99%", decimals=0)
```

```{r, eval=FALSE}
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q95(data$ADD), removed=q95(data$DEL), complexity=q95(data$COMPLEX), duplicates = q95(data$DUP), summary=function(x) q99(x)), "Quantile 99%", decimals=0)
```

```{r, eval=FALSE}
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=q99(data$ADD), removed=q99(data$DEL), complexity=q99(data$COMPLEX), duplicates = q99(data$DUP), summary=function(x) quantile(x, 0.75)), "Quantile 75%", decimals=0)
```

```{r, eval=FALSE}
heatmap_by_team_and_repo(posterior_predict_by_team_and_repo(m, added=quantile(data$ADD, 0.99), removed=quantile(data$DEL, 0.1), complexity=mean(data$COMPLEX), duplicates = mean(data$DUP), summary=function(x) median(x)), "Median a posteriori", decimals=0)

```

```{r, eval=FALSE}
loo_compare(M_added_removed_lines, M_crossed_team_repo_complex_dup_repo_pop, M_crossed_team_repo_model, M_crossed_team_team_repo_model, M_crossed_team_team_repo_zi_model, M_intercepts_only)
```

```{r, eval=FALSE}
summary(m)
```

```{r, eval=FALSE}
source("conditional_effects.R")
```

```{r, eval=FALSE}
teamBlue <- condeffect_logADD_by_logCOMPLEX(m, d, "Blue", "Jupiter")
```
```{r, eval=FALSE}
teamArch <- condeffect_logADD_by_logCOMPLEX(m, d, "Arch", "Jupiter")
```

```{r, eval=FALSE}
plot_logADD_by_logCOMPLEX(m, d, teamBlue)
```
```{r, eval=FALSE}
library(forcats)
reloo_plot_logADD_by_logCOMPLEX <- function(reloo, someData, ftot, aTeam, aRepo) {
  tmp <- bind_cols(someData, reloo$diagnostics) |> mutate(
    truncC=factor(trunc(unscale_complexity(round(C)))), 
    added=unscale_added(A), 
    complexity=factor(trunc(unscale_complexity(round(C)))))
  sorted_labels <- paste(sort(as.integer(levels(tmp$truncC))))
  tmp$truncC <- factor(tmp$truncC, levels = sorted_labels)
  tmp$complexity <- factor(tmp$complexity, levels = sorted_labels)
  
  observed <- tmp |> filter(team == aTeam, repo == aRepo)
  
  tmp <- ftot |> mutate(added=unscale_added(A),
                        complexity=factor(trunc(unscale_complexity(as.integer(C))), levels=trunc(unscale_complexity(C)))) 
  predicted <- tmp
  return(predicted |> ggplot(aes(x=added)) +
    geom_smooth(aes(y=Estimate, ymin=Q5.5, ymax=Q94.5, group=complexity, color=complexity), stat="identity", alpha=.25, linewidth=.5) +
    geom_point(data=observed, aes(y=y, size = pareto_k, color=truncC), alpha=0.2) +
      ggtitle(paste0("Conditional effects of team ", aTeam, " in repo ", aRepo))
  )
}
reloo_plot_logADD_by_logCOMPLEX(reloo, d, teamBlue, "Blue", "Jupiter")
```

```{r, eval=FALSE}
teamBlue |> mutate(added=unscale_added(A), complexity=factor(trunc(unscale_complexity(as.integer(C))))) |> select(complexity) |> summary()
```


```{r, eval=FALSE}
d |> sample_n(10) |> select(y, A, C, team, repo) |> mutate(foo = trunc(unscale_complexity(round(C))))
  #mutate(truncC=factor(trunc(unscale_complexity(round(C, 0)))), added=unscale_added(A), complexity=factor(trunc(unscale_complexity(round(C, 0))))) |> select(complexity) 
```



```{r, eval=FALSE}
plot_logADD_by_logCOMPLEX(m, d, teamArch, "Arch", "Jupiter")

```
```{r, eval=FALSE}
plot_logADD_by_logCOMPLEX(m, d, teamBrown, "Brown", "IntTest")

```

```{r, eval=FALSE}
teamUI <- condeffect_logADD_by_logCOMPLEX(m, d, "UI", "Jupiter")
```

```{r, eval=FALSE}
plot_logADD_by_logCOMPLEX(m, d, teamUI, "UI", "Jupiter")

```

```{r, eval=FALSE}
teamBrown <- condeffect_logADD_by_logCOMPLEX(m, d, "Brown", "IntTest")
```

```{r, eval=FALSE}
reloo_plot_logADD_by_logCOMPLEX(reloo, d, teamBrown, "Brown", "IntTest")

```

```{r, eval=FALSE}
mcmc_areas(m, regex_pars = c("^b_"))
```

```{r, eval=FALSE}
mcmc_areas(m, regex_pars = c("^sd_"))

```

```{r, eval=FALSE}
mcmc_areas(m, regex_pars = c("^r_team[[].*,Intercept"))

```

```{r, eval=FALSE}
mcmc_areas(m, regex_pars = c("^r_team[[].*,A[]]"))

```

```{r, eval=FALSE}
mcmc_areas(m, regex_pars = c("^r_team:repo[[].*IntTest,A[]]"))

```

