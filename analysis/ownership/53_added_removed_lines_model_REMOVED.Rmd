---
title: "53 Existing duplicates, added and removed lines model for removals"
author: "Anders Sundelin"
date: "2023-03-19"
output: html_document
params: 
    cache: "../.cache"
    output: "../ownership/output"
    reloo: FALSE
    cores: 2
    threads: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(tidyr)
library(dplyr)
library(brms)
library(bayesplot)
library(patchwork)
```

```{r}
set.seed(12345)
source("ingest_data.R")
MODEL_CACHE <- "removed-M_added_removed_lines"
```

# Settings
Models are stored in the following directory, which must exist prior to knitting this document:

```{r}
cat(normalizePath(paste(getwd(), dirname(cachefile(MODEL_CACHE)), sep="/"), mustWork = T))
```

The used cache directory can be controlled via the cache parameter to Rmd - it can be useful to experiment with this parameter if you Knit the document manually in RStudio.

# Existing-duplicates, plus change size model

As we are modeling removals of duplicates, it seems logical that the number of existing duplicates in a file would affect the probability of duplicates being removed.
After all, if the file contains no duplicates, it will be impossible to remove any, no matter how much you change the file.
For the team and team-in-repository components, we will use added and removed lines as predictor.

This leaves us with the following model:

```{r}
d <- data |> select(y=REMOVED,
                    A=A,
                    C=C,
                    D=D,
                    R=R,
                    team=committerteam,
                    repo=repo)
formula <- bf(y ~ 1 + D + A + R + (1 + A + R | team) + (1 + A + R | team:repo),
              zi ~ 1 + D + A + R + (1 + A + R | team) + (1 + A + R | team:repo))
get_prior(data=d,
          family=zero_inflated_negbinomial,
          formula=formula)
```
```{r}
priors <- c(prior(normal(0, 0.5), class = Intercept),
            prior(normal(0, 0.25), class = b),
            prior(weibull(2, 0.25), class = sd),
            prior(lkj(2), class = cor),
            prior(normal(0, 0.5), class = Intercept, dpar=zi),
            prior(normal(0, 0.25), class = b, dpar=zi),
            prior(weibull(2, 0.25), class = sd, dpar=zi),
            prior(gamma(0.5, 0.1), class = shape))

(v <- validate_prior(prior=priors,
               formula=formula,
               data=d,
               family=zero_inflated_negbinomial)
)
```

```{r}
M_ppc <- brm(data = d,
      family = zero_inflated_negbinomial,
      formula = formula,
      prior = priors,
      file = cachefile(paste0("ppc-",MODEL_CACHE)),
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      sample_prior = "only",
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)
```


```{r}
m <- M_ppc
```


```{r prior_predict}
yrep <- posterior_predict(m)
```

### Number of zeros

```{r ppc_zeros}
ppc_stat(y = d$y, yrep, stat = function(y) mean(y == 0)) + ggtitle("Prior predicted proportion of zeros")
```

The proportion of zeros seem plausible.

### Max predicted value

```{r ppc_max}
(sim_max <- ppc_stat(y = d$y, yrep, stat = "max") + ggtitle("Prior predicted max values")
)
```

```{r}
sim_max + scale_x_continuous(limits = c(0,1000)) + ggtitle("Prior predicted max values up to 1000")
```

Maximum predictions is slightly wider than the intercept model, but the observed count fall within our prior predictions.

### 99th percentile

```{r ppc_q99}
ppc_stat(y = d$y, yrep, stat = "q99") + ggtitle("Prior predicted Q99 vs. observed value")
```

Relative to the intercept model, our priors place a little more probability on seeing duplicates being removed.
The range is similar, though, it is just the distribution that changes.

### 99th vs 95th percentile

```{r ppc_2d}
(p <- ppc_stat_2d(d$y, yrep, stat = c("q95", "q99")) + theme(legend.position="bottom") + ggtitle("Prior predicted Q95 vs Q99")
)
```

Similar-looking 95th-vs-99th-percentile plot as the intercept model.

### Standard deviation

```{r ppc_sd}
(p <- ppc_stat(y = d$y, yrep, stat = "sd") + ggtitle("Prior predicted stddev vs. observed value")
)
```

Relative to the intercept model, this model has a slightly wider standard deviation range.

```{r}
p + scale_x_continuous(limits=c(0,30))
```

### Group-level predictions

```{r ppc_grouped_team}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$team) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per team")
```

Wider prior predictions for most groups with these priors (Pink team being the exception).

```{r ppc_grouped_repo}
ppc_stat_grouped(y = d$y, yrep, stat = "q99", group = d$repo) + theme(legend.position = "bottom") + ggtitle("Prior predictive Q99 observation per repository")
```

When grouping per repository, there is much larger variation.
Note that our model does not have any repository-only predictor, only "team-in-repository" (the reason being that a repository is not acting on its own---all changes are performed by a team).

## Model execution and diagnostics

```{r model_execution}
M_added_removed_lines <- brm(data = d,
      family = zero_inflated_negbinomial,
      file = cachefile(MODEL_CACHE),
      formula = formula,
      prior = priors,
      warmup = 1000,
      iter  = ITERATIONS,
      chains = CHAINS,
      cores = CORES,
      backend="cmdstanr",
      file_refit = "on_change",
      threads = threading(THREADS),
      save_pars = SAVE_PARS,
      adapt_delta = ADAPT_DELTA)

```

```{r loo}
M_added_removed_lines <- add_criterion(M_added_removed_lines, "loo")
```

```{r}
m <- M_added_removed_lines
```


```{r mcmc_trace}
p <- mcmc_trace(m)
pars <- levels(p[["data"]][["parameter"]])
plots <- seq(1, to=length(pars), by=12)
lapply(plots, function(i) {
  start <- i
  end <- start+11
  mcmc_trace(m, pars = na.omit(pars[start:end]))
  })
```

Now we run into problem with the model convergence---we see that certain chains did not converge properly.
For instance, the `b_D` and `b_zi_D` predictors did not converge at all, and the intercepts are also fishy, as is the shape.


```{r, rhat_neff}
mcmc_plot(m, type="rhat")
mcmc_plot(m, type="rhat_hist")
mcmc_plot(m, type="neff")
mcmc_plot(m, type="neff_hist")
```

We see also that some $\hat{R}$ values are much higher than 1.

This means that the model did not converge properly, and we should not trust the posterior predictions.

```{r plot_loo}
loo <- loo(m)
loo
plot(loo)
```

